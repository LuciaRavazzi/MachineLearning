{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> NLP WITH BERT </h1></center>\n",
    "\n",
    "Bert is a pre-trained model made by Google which implement sentimental analysis for the movie review: predict if they are positive or negative.\n",
    "\n",
    "What is a **_pre-trained_** method? For example, If you want to train a NN, any NN, the initial layers are going to detect slant lines no matter what you want to classify. Hence, it does not make sense to train them every time you create a neural network.\n",
    "It is only the final layers of our network, the layers that learn to identify classes specific to your project that need training.\n",
    "Hence what we do is, we take a Resnet34, and remove its final layers. We then add some layers of our own to it (randomly initialized) and train this new model. So, the core is to add a neural network to the pretrained model or, if it's necessary, to drop out some layers and add some new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain # it's a huge package\n",
    "from ktrain import text\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 500\n",
      "done.\n",
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 2e-05...\n",
      " 216/4167 [>.............................] - ETA: 59:50:40 - loss: 0.5550 - accuracy: 0.7284"
     ]
    }
   ],
   "source": [
    "# Downloads a file from a URL if it not already in the cache in a folder of keras.\n",
    "dataset_path = tf.keras.utils.get_file(fname = \"aclImdb_v1.tar.gz\",\n",
    "                                 origin = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "                                 extract=True \n",
    "                                 )\n",
    "\n",
    "# Retrieve the path of file from the root.\n",
    "IMDB_DATADIR = os.path.join(os.path.dirname(dataset_path), # mi d√† la directory che contiene il dataset.\n",
    "                            'aclImdb')\n",
    "\n",
    "# The function returns the train, test set and the preprocessing method of the text.\n",
    "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder(datadir=IMDB_DATADIR,\n",
    "                                                                       classes=['pos','neg'], # names of the folder.\n",
    "                                                                       maxlen=500, # max number of word in text (reasonable for a review).\n",
    "                                                                       train_test_names=['train','test'], # names in te folder.\n",
    "                                                                       preprocess_mode='bert'\n",
    "                                                                      )\n",
    "\n",
    "# Build the BERT (Classifiation) model.\n",
    "model = text.text_classifier(name = 'bert',\n",
    "                            train_data = (x_train, y_train),\n",
    "                            preproc = preproc)\n",
    "\n",
    "# Training the BERT model.\n",
    "learner = ktrain.get_learner(model=model,\n",
    "                             train_data=(x_train, y_train),\n",
    "                             val_data=(x_test, y_test),\n",
    "                             batch_size=6\n",
    "                            )\n",
    "\n",
    "learner.fit_onecycle(lr=2e-5, # learning rate.\n",
    "                     epochs=1 # it's very slow to perform.\n",
    "                    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
