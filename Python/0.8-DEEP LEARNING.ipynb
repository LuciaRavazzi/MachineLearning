{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> DEEP LEARNING </h3></center>\n",
    "<h3><center> ARTIFICIAL NEURAL NETWORK </h3></center>\n",
    "M. Nielsen, Neaural network http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "\n",
    "Neural network born in 80s. However, there wasn't the computational power and the data to develop them.\n",
    "The weight of DNA needed to store the world information is 1kg! The cost of memory decreased over time while the availability of memory and its cost are accessible to everyone. \n",
    "What is deep learning? Hinton is the father of deep leraning, it's a reaserach in 80s. The main idea is the human brain and that network want to recreate its connections. So, we want to recreate that structure with these chaarcteristics:\n",
    "- Input layer (eyes, hears in biology)\n",
    "- Output layer \n",
    "- Hidden Layer. Shallow learning (a few hidden layers) vs. deep learning (high value of them like the human brain).\n",
    "But even if we are able to build a neural network (it's possible with the Moore's law), it won't be as the human brain because the absence of semantic intelligence.\n",
    "\n",
    "The neural network is characterized by:\n",
    "- **_Neuron_** is the basic building block of neural network. We want to mimic the neuron of the brain. Each neuron is useless like only an ant: only if there are lots neurons, the brain acquires its powerful abilities. In computer science, the neuron is a node which receives an input signal and gives an output signal. It's important that the independent variable are _standardized_ in order not to have the overcame of one over others. The output value can be continuous, binary or categorical (in this case there are more than one output variable since the result is transformed into dummy variable). Links between nodes are defined by a weight $w_i$ which are defined in the training phase. The signal of input is valuated into the node by a _activation function_ $$\\phi(\\sum_{i=1}^{N} w_i x_i)$$\n",
    "- **_Activation function_**: $\\phi$ is the activation function:\n",
    "   - Threshold function $\\phi(z) = \\begin{cases} \\, 1 \\, if \\, z > 0 \\\\ 0 \\, if \\, z \\leq 0\\end{cases}$. For binary data.\n",
    "   - Sigmoid function: $\\phi(z) = \\frac{1}{1+e^{-z}}$. It's smooth unlike the previous one. It's used for the last neuron in order to have a probability. For binary data (for their probability).\n",
    "   - Rectifier: $\\phi(z) = max(z,0)$ is the most used function in ANN.\n",
    "   - Hyperblocic tangent: $\\phi(z) = \\frac{1-e^{-2z}}{1+e^{-2z}}$\n",
    "   It's common to use the rectifier function in the hidden layers and finally, the sigmoid one in order to output a probability.\n",
    "- **_How the neural network work?_** (testing phase). If there aren't hidden layers, the neural network can be seen as a regression, e.g. multiple, simple or multivariate. However, they have much more flexibility and power due to the use of hidden layers.  For the test phase, a new record feeds the network in order to obtain the result.\n",
    "- **_How the neural network learn?_**. There are two ways: the first one concerns the code which defines the rule adopted by the network, on the other hand, the network create a facility defined by the weight. Our goal is to create the network and not to define the set of rules. **Perceptron** or **single feedforward neural network** (since it haven't cicles unlike the _Recurrent neural network_) is defined by only one neuron. The training phase is carried out with the training set by which the weights of nodes are built with the minimization of a _loss function_ from the label given by the dataset. The difference between the predicted label and the true label is used to give a value to the weights with a backforward approach. The update of  the weights depends on the type of chosen technique (GD or SGD). The _**backpropagation**_ is most common approach to learn the network. One _epoch_ is the training phase of the network with one dataset. \n",
    "- **_Gradient descent or Batch Gradient descent_** is a method which aim to find the stationary point of a _loss function_, e.g. $C = \\frac{1}{2} (\\hat{y}-y)^2$. _Curse of dimensionality_ is mandatory if the number of weights are too many. The _brutal force approach_ aims to find the minimum of the function with the combinations of the parameters. In this case, only the _total cost_ obtained by an all dataset is used to reweight the parameters. It's possible to perform a _mini-batch DM_, i.e. a mix between GD and SGD. \n",
    "- **_Stochastic gradient descent_**. The previous function requires a convex function that can be a problem which is overcome by this method. So, with the first it's possible to get caught in a local minimum. Unlike the previous method, weights are adjusted after the training of each single row. It ahs much more fluctuation with respect to the GD and it's slower. \n",
    "- **_Backpropagation_** refres to the propagation in the input direction after computing the error in order to adjust the weights. \n",
    "\n",
    "Steps by steps of networks:\n",
    "- Randomly initialise the weughts to small number close, but different, to zero.\n",
    "- Input the first observation of the dataset on the input layer.\n",
    "- Forward-propagation. \n",
    "- Compare the predicted and the true result.\n",
    "- Back-propagation: the error is back-propagated, update the weights according to how much they are responsible for this error. The lerning rate decides how much we update the weights.\n",
    "- Repeat the previous steps after each obseravtion (SGD learning), , reweights only after a batch of observation (batch learning)\n",
    "- Repeat the epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Keras was integrated in tf\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHURN PROBLEM: CLASSIFICATION TASK (predict the probability of being a churner).\n",
    "\n",
    "#--------------- PREPROCESSING\n",
    "# Import the dataset.\n",
    "dataset = pd.read_csv('Dataset/Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values\n",
    "y = dataset.iloc[:,-1].values\n",
    "\n",
    "# Encoding categorical variable.\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "LE = LabelEncoder()\n",
    "y = LE.fit_transform(y)\n",
    "# encode the gender.\n",
    "X[:,2] = LE.fit_transform(X[:,2])\n",
    "\n",
    "ct = ColumnTransformer(transformers =[('encoder', OneHotEncoder(), [1])], \n",
    "                       remainder = 'passthrough' ) \n",
    "X = np.array(ct.fit_transform(X)); \n",
    "\n",
    "# SPLIT THE DATASET.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, # independent variables.\n",
    "                                                    y, # dependent variables.\n",
    "                                                    test_size = 0.2, # size of test set.\n",
    "                                                    random_state = 1) # fix the seed for both of data set.\n",
    "\n",
    "\n",
    "# FEATURE SCALING: it's compulsory for deep learning regardless the presence of binary variable.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.6278 - accuracy: 0.6658\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 977us/step - loss: 0.4807 - accuracy: 0.7975\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 910us/step - loss: 0.4426 - accuracy: 0.8010\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 930us/step - loss: 0.4161 - accuracy: 0.81550s - loss: 0.4194 - accuracy: 0.\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 898us/step - loss: 0.3976 - accuracy: 0.8326\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 890us/step - loss: 0.3850 - accuracy: 0.8382\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 925us/step - loss: 0.3761 - accuracy: 0.8426\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 902us/step - loss: 0.3698 - accuracy: 0.8457\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 946us/step - loss: 0.3646 - accuracy: 0.8493\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 994us/step - loss: 0.3604 - accuracy: 0.8511\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8518\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3561 - accuracy: 0.8521\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 945us/step - loss: 0.3544 - accuracy: 0.8516\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 926us/step - loss: 0.3531 - accuracy: 0.8524\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 951us/step - loss: 0.3529 - accuracy: 0.8530\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 974us/step - loss: 0.3516 - accuracy: 0.8537\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 945us/step - loss: 0.3509 - accuracy: 0.8533\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 941us/step - loss: 0.3506 - accuracy: 0.8550\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 962us/step - loss: 0.3499 - accuracy: 0.8545\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.3495 - accuracy: 0.8549\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 918us/step - loss: 0.3488 - accuracy: 0.8564\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 958us/step - loss: 0.3485 - accuracy: 0.8558\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.3480 - accuracy: 0.8553\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 929us/step - loss: 0.3477 - accuracy: 0.8551\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 906us/step - loss: 0.3470 - accuracy: 0.8568\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 922us/step - loss: 0.3466 - accuracy: 0.8569\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 938us/step - loss: 0.3467 - accuracy: 0.8569\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 957us/step - loss: 0.3464 - accuracy: 0.8566\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 917us/step - loss: 0.3458 - accuracy: 0.8568\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 934us/step - loss: 0.3458 - accuracy: 0.8565\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 942us/step - loss: 0.3453 - accuracy: 0.8566\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 914us/step - loss: 0.3454 - accuracy: 0.8574\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3452 - accuracy: 0.8577: 0s - loss: 0.3484 \n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8584\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 850us/step - loss: 0.3444 - accuracy: 0.8584\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 866us/step - loss: 0.3444 - accuracy: 0.8577\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 814us/step - loss: 0.3438 - accuracy: 0.8586\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 858us/step - loss: 0.3437 - accuracy: 0.8587\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3433 - accuracy: 0.8597\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 838us/step - loss: 0.3432 - accuracy: 0.8597\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 814us/step - loss: 0.3433 - accuracy: 0.8599\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 826us/step - loss: 0.3428 - accuracy: 0.8601\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 918us/step - loss: 0.3429 - accuracy: 0.8597\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 961us/step - loss: 0.3425 - accuracy: 0.8602\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 830us/step - loss: 0.3422 - accuracy: 0.8593\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 834us/step - loss: 0.3423 - accuracy: 0.8604\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 810us/step - loss: 0.3422 - accuracy: 0.8585\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 858us/step - loss: 0.3416 - accuracy: 0.8604\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 806us/step - loss: 0.3417 - accuracy: 0.8616\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 802us/step - loss: 0.3413 - accuracy: 0.8606\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 846us/step - loss: 0.3413 - accuracy: 0.8599\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 862us/step - loss: 0.3413 - accuracy: 0.8601\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8611\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3405 - accuracy: 0.8596\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8601\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.8601\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 906us/step - loss: 0.3399 - accuracy: 0.8604\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 961us/step - loss: 0.3400 - accuracy: 0.8602\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3395 - accuracy: 0.8612\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 981us/step - loss: 0.3401 - accuracy: 0.8614\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 914us/step - loss: 0.3395 - accuracy: 0.8601\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 914us/step - loss: 0.3391 - accuracy: 0.8611\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 906us/step - loss: 0.3387 - accuracy: 0.86160s - loss: 0.3355 - accuracy\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 926us/step - loss: 0.3386 - accuracy: 0.8634\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 890us/step - loss: 0.3388 - accuracy: 0.8616\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 914us/step - loss: 0.3386 - accuracy: 0.8615\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 922us/step - loss: 0.3379 - accuracy: 0.8633\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 934us/step - loss: 0.3379 - accuracy: 0.8634\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 906us/step - loss: 0.3382 - accuracy: 0.8618\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 945us/step - loss: 0.3374 - accuracy: 0.8615\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 974us/step - loss: 0.3377 - accuracy: 0.8620\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.8620\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 878us/step - loss: 0.3370 - accuracy: 0.8627\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 890us/step - loss: 0.3370 - accuracy: 0.8630\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3366 - accuracy: 0.8614\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8631\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8624\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.8608\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8633\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.8637: 0s - loss: 0.3408 - accuracy: 0.\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 965us/step - loss: 0.3357 - accuracy: 0.8621\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8633\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8630\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 850us/step - loss: 0.3359 - accuracy: 0.8620\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 830us/step - loss: 0.3352 - accuracy: 0.8602\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 850us/step - loss: 0.3353 - accuracy: 0.8630\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 854us/step - loss: 0.3355 - accuracy: 0.8620\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 858us/step - loss: 0.3357 - accuracy: 0.8634\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 876us/step - loss: 0.3353 - accuracy: 0.8612\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3346 - accuracy: 0.8633: 0s - loss: 0.3468 - \n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 997us/step - loss: 0.3351 - accuracy: 0.8610\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 993us/step - loss: 0.3349 - accuracy: 0.8629\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.3347 - accuracy: 0.8633\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8605\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8614\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 918us/step - loss: 0.3346 - accuracy: 0.8615\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 850us/step - loss: 0.3350 - accuracy: 0.8620\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 866us/step - loss: 0.3345 - accuracy: 0.8616\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 834us/step - loss: 0.3343 - accuracy: 0.8618\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 834us/step - loss: 0.3345 - accuracy: 0.8608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e90f63e700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------- BUILD THE NEURAL NETWORK\n",
    "# The network is created as the instance of a class, i.e. object\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "# the number of nodes aren't defined by a rule of thumb but with experience.\n",
    "number_first = 6\n",
    "# it's a shallow neural network... to have a deep network more layers must be added.\n",
    "first_layer = tf.keras.layers.Dense(units = number_first, # numer of neurons\n",
    "                                    activation = 'relu' # it's the best for hidden layers (rectifier).\n",
    "                                   )\n",
    "ann.add(first_layer)\n",
    "\n",
    "# deep neural network.\n",
    "second_layer = tf.keras.layers.Dense(units = number_first, # numer of neurons\n",
    "                                    activation = 'relu' # it's the best for hidden layers (rectifier).\n",
    "                                   )\n",
    "ann.add(second_layer)\n",
    "\n",
    "# output neural network.\n",
    "number_output = 1\n",
    "output_layer = tf.keras.layers.Dense(units = number_output, # numer of neurons\n",
    "                                    activation = 'sigmoid' # it's the best for hidden layers (rectifier).\n",
    "                                   )\n",
    "ann.add(output_layer)\n",
    "\n",
    "# COMPILE THE NETWORK.\n",
    "ann.compile(optimizer = 'adam', # technique which perform the optimization (such as Gradient Descent).)\n",
    "            loss = 'binary_crossentropy', # in binary classification, the most used is binary_crossentropy.\n",
    "            metrics = ['accuracy'] # measure to understand the quality of network.\n",
    "           )\n",
    "\n",
    "# TRAIN THE NETWORK.\n",
    "ann.fit(X_train, \n",
    "        y_train, \n",
    "        batch_size = 32, # comparison between the true and the redicted value occur before batch_size\n",
    "       epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8585"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict a new value.\n",
    "\n",
    "ann.predict(sc.transform([[1,0,0,600,1,40,3,60000,2,1,1,50000]])) > 0.5\n",
    "\n",
    "# Accuracy.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = y_pred > 0.5 \n",
    "confusion_matrix(y_test, y_pred)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\lucia\\anaconda3\\lib\\site-packages (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Lucia\\\\Anaconda3\\\\lib\\\\site-packages\\\\ann_visualizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ann_visualizer\n",
    "ann_visualizer.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install ann_visualizer\n",
    "# !pip install graphviz\n",
    "\n",
    "# Il pacchetto inizialmente non funzionava perché era necessario \n",
    "# scaricare graphviz ed aggiungere il percorso degli eseguibili alle variabili\n",
    "# d'ambiente di sistema (di tutti gli utentu) e del mio utente (solo io).\n",
    "from ann_visualizer.visualize import ann_viz;\n",
    "ann_viz(ann, title=\"My first neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center> CONVOLUTIONAL NEURAL NETWORK </h3></center>\n",
    "\n",
    "Our brain classify images on the basis of the characteristics that it's able to catch. CNN classifies in a similar way. It's used to tag images or for the car.\n",
    "The convolutional neural netwroks are built with the following steps:\n",
    "- Convolutional\n",
    "- Max pooling\n",
    "- Flattening\n",
    "- Full connection: build an artificial neural network.\n",
    "\n",
    "Reccomended articles: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf, https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\n",
    "\n",
    "We will study the following concepts:\n",
    "- **_What are convolutional neural networks?_**. They input an image, there is a CNN and that image is classified. The important thing is the feature, indeed the network can function in a wrong way if it doesn't see much of them. One white/black image is a 2d array (0 or 1 for each pixel), while the colour one is a 3d array (RGB for each pixel). \n",
    "- **_Convolutional operation_**: \n",
    "the convolutional function is $(f*g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau) dt$. \n",
    "The **feature detector or kernel or filter** is a 3x3 matrix which gives the **feature or activation map** when we compute the product between the input image and the feature detector by which we extract only one value. \n",
    "So, in this manner, we reduce the dimension of the input image. \n",
    "On the other hand, we are loosing information but we want to detect _the_ feature detected by the filter. Indeed, the higher is the result, the higher is presence of that caracteristic in that part of the image. \n",
    "The first layer of convolutional map is a set of **feature maps**, each of them represent a feature of the image which is associated to a different _feature detector_. There are different types of filetrs, such as sharpen (the middle of the detector is the most dense), blur, edge enhance, edge detect, emboss.\n",
    "- **_RELU layer_**. The reason why we want to apply this function is to enhance the _non-linearity_ in our image because images are highly non-linear elements . We can consider this step as one sub-step of the previous one. \n",
    "- **_Pooling_**. It's used to learn the **invariance** of our information in order to generalize better. There are _max_, _min_ or _average_ pooling. In the first one, given a sub-matrix of the feature map, that function select the max of that matrix. The most important fact is that the **pooled feature map** is invariant after the rotation or some distortions of the image (such as the tears of the cheetah) and also we are reducing the number of parameter preventing overfitting. \n",
    "\n",
    "Interesting article: http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf\n",
    "- **_Flattening_**: transform the matrix into a vector, it can be interpretated as the input layer of an ANN.\n",
    "- **_Full connection_**: we are adding an artificial neural network after the convolutional, pooling and flattening. In this section, we are defining the hidden layer as the **fully connected layer**. The dimension of input is the number of features which I consider. The learning process is performed as usually. \n",
    "- **_Softmax_**: it's a generalization of logistic function: $$f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$$\n",
    "- **_Cross entropy_** $$L_i= -log(\\frac{e^{f_{y_i}}}{\\sum_k e^{f_j}})$$ which is connected to $$H(p,q) = -\\sum_x p(x) log(q(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Google developed tensorflow, while Facebook developed keras. \n",
    "# Python is better due to the presence of different libraries to perform these networks.\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Tranformation of the training test in order \n",
    "# avoid overfitting, such as geometrical transformation or zooming. \n",
    "\n",
    "# IMAGE AUGMENTATION: we add a generalization to our images.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,   # feature scaling to each pixel (normalization). \n",
    "    shear_range = 0.2,  \n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('Dataset/dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary' # outcome.\n",
    "                                                )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('Dataset/dataset/test_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary' # outcome.\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 134s 537ms/step - loss: 0.6897 - accuracy: 0.5559 - val_loss: 0.6646 - val_accuracy: 0.5950\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 102s 409ms/step - loss: 0.6583 - accuracy: 0.6180 - val_loss: 0.6429 - val_accuracy: 0.6335\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 71s 285ms/step - loss: 0.6297 - accuracy: 0.6481 - val_loss: 0.6024 - val_accuracy: 0.6810\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 71s 285ms/step - loss: 0.5859 - accuracy: 0.6923 - val_loss: 0.5626 - val_accuracy: 0.7210\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 66s 263ms/step - loss: 0.5548 - accuracy: 0.7089 - val_loss: 0.5545 - val_accuracy: 0.7215\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.5224 - accuracy: 0.7385 - val_loss: 0.5111 - val_accuracy: 0.7495\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 0.4975 - accuracy: 0.7538 - val_loss: 0.5063 - val_accuracy: 0.7580\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.4733 - accuracy: 0.7716 - val_loss: 0.5019 - val_accuracy: 0.7490\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.4516 - accuracy: 0.7859 - val_loss: 0.4782 - val_accuracy: 0.7925\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 77s 309ms/step - loss: 0.4393 - accuracy: 0.7935 - val_loss: 0.4690 - val_accuracy: 0.7870\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.4278 - accuracy: 0.8010 - val_loss: 0.4458 - val_accuracy: 0.7990\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 0.4021 - accuracy: 0.8127 - val_loss: 0.4621 - val_accuracy: 0.7810\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 64s 255ms/step - loss: 0.3943 - accuracy: 0.8209 - val_loss: 0.4563 - val_accuracy: 0.7980\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 77s 310ms/step - loss: 0.3749 - accuracy: 0.8270 - val_loss: 0.4750 - val_accuracy: 0.7770\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.3598 - accuracy: 0.8371 - val_loss: 0.4476 - val_accuracy: 0.8100\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 83s 330ms/step - loss: 0.3429 - accuracy: 0.8453 - val_loss: 0.4430 - val_accuracy: 0.8115\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 66s 265ms/step - loss: 0.3222 - accuracy: 0.8624 - val_loss: 0.4432 - val_accuracy: 0.8170\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 64s 258ms/step - loss: 0.3169 - accuracy: 0.8634 - val_loss: 0.4654 - val_accuracy: 0.8015\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.2979 - accuracy: 0.8691 - val_loss: 0.4511 - val_accuracy: 0.8065\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 50s 202ms/step - loss: 0.2754 - accuracy: 0.8855 - val_loss: 0.4838 - val_accuracy: 0.8045\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.2611 - accuracy: 0.8880 - val_loss: 0.5100 - val_accuracy: 0.7875\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 56s 225ms/step - loss: 0.2534 - accuracy: 0.8932 - val_loss: 0.5075 - val_accuracy: 0.7970\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.2391 - accuracy: 0.8994 - val_loss: 0.5152 - val_accuracy: 0.8050\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 56s 225ms/step - loss: 0.2388 - accuracy: 0.9024 - val_loss: 0.5034 - val_accuracy: 0.8030\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 56s 225ms/step - loss: 0.2185 - accuracy: 0.9091 - val_loss: 0.5347 - val_accuracy: 0.7885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ef392d448>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUILD THE NETWORK.\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# First Convolutional layer.\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, # the number of output filters in the convolution\n",
    "                              kernel_size = 3, # specifying the height and width of the 2D convolution window.\n",
    "                              activation = 'relu',\n",
    "                              input_shape = [64, 64, 3]))\n",
    "\n",
    "# First Pooling.\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, # dimensio n of matrix.\n",
    "                                 strides = 2,    # step of matrix.\n",
    "                                 ))\n",
    "\n",
    "# Second convolutional layer.\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, # the number of output filters in the convolution\n",
    "                              kernel_size = 3, # specifying the height and width of the 2D convolution window.\n",
    "                              activation = 'relu'\n",
    "                              ))\n",
    "\n",
    "# Second pooling.\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, # dimensio n of matrix.\n",
    "                                 strides = 2,    # step of matrix.\n",
    "                                 ))\n",
    "\n",
    "# Flattening.\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Full connection.\n",
    "number_first = 128 # for images it's necessary to have several nodes.\n",
    "first_layer = tf.keras.layers.Dense(units = number_first, # numer of neurons\n",
    "                                    activation = 'relu' # it's the best for hidden layers (rectifier).\n",
    "                                   )\n",
    "cnn.add(first_layer)\n",
    "\n",
    "# output neural network.\n",
    "number_output = 1\n",
    "output_layer = tf.keras.layers.Dense(units = number_output, # numer of neurons\n",
    "                                    activation = 'sigmoid' # it's the best for hidden layers (rectifier).\n",
    "                                   )\n",
    "cnn.add(output_layer)\n",
    "\n",
    "# COMPILE THE NETWORK.\n",
    "cnn.compile(optimizer = 'adam', # technique which perform the optimization (such as Gradient Descent).)\n",
    "            loss = 'binary_crossentropy', # in binary classification, the most used is binary_crossentropy.\n",
    "            metrics = ['accuracy'] # measure to understand the quality of network.\n",
    "           )\n",
    "\n",
    "# Train.\n",
    "# TRAIN THE NETWORK.\n",
    "cnn.fit(x = training_set, \n",
    "        validation_data = test_set, \n",
    "        batch_size = 32, # comparison between the true and the redicted value occur before batch_size\n",
    "        epochs = 25)\n",
    "\n",
    "# INTERESTING OBSERVATION: WE DON'T GIVE TO THE ALGORITHM THE LABELS!!\n",
    "# iT'S ABLE TO RECOGNIZE THEM ALONE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "\n",
    "test_image = image.load_img('Dataset/dataset/single_prediction/cat_or_dog_1.jpg', \n",
    "                            target_size = (64, 64))\n",
    "# from PIL format to numpy format.\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "# le etichette derivano dai file i quali\n",
    "# si riferiscono ad immagini già etichettate.\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1: # dog\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA_ROOT_ENV",
   "language": "python",
   "name": "conda_root_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
