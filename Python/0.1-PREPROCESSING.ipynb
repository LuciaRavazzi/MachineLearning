{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> PREPROCESSING </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to visualize information of function, push Alt+Shift o Alt+Shift+Shift\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 1.0, -0.19159184384578545, -1.0781259408412425],\n",
       "       [0.0, 1.0, 0.0, -0.014117293757057777, -0.07013167641635372],\n",
       "       [1.0, 0.0, 0.0, 0.566708506533324, 0.633562432710455],\n",
       "       [0.0, 0.0, 1.0, -0.30453019390224867, -0.30786617274297867],\n",
       "       [0.0, 0.0, 1.0, -1.9018011447007988, -1.420463615551582],\n",
       "       [1.0, 0.0, 0.0, 1.1475343068237058, 1.232653363453549],\n",
       "       [0.0, 1.0, 0.0, 1.4379472069688968, 1.5749910381638885],\n",
       "       [1.0, 0.0, 0.0, -0.7401495441200351, -0.5646194287757332]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------- IMPORT DATASET.\n",
    "dataset = pd.read_csv('Dataset/DataPreprocessing.csv')\n",
    "\n",
    "#-------- SPLIT THE COLUMN FOR THE DEPENDENT AND INDEPENDENT VARIABLES.\n",
    "# iloc: location index.\n",
    "# senza values è una tabella, con è un array.\n",
    "X = dataset.iloc[:,:-1].values; \n",
    "y = dataset.iloc[:,-1].values; \n",
    "\n",
    "#-------- MISSING VALUES\n",
    "# FIRST APPROACH: RECORD REMOVAL.\n",
    "# drop the records function if the dataset is large and the \n",
    "# percentage is low.\n",
    "# or REPLACE missing values with mean, median etc.\n",
    "from sklearn.impute import SimpleImputer as SI\n",
    "\n",
    "imputer = SI(missing_values = np.nan, strategy = 'mean')\n",
    "imputer.fit(X[:,1:3])\n",
    "X[:,1:3] = imputer.transform(X[:,1:3]); X\n",
    "\n",
    "#-------- ONE HOT ENDING\n",
    "# TURNING A COLUMN INTO DIFFERENT COLUMNS.\n",
    "# INDEED, IF WE ENCODE WITH NUMBER, WE INTRODUCE AN ORDER THAT DOESN'T EXIST.\n",
    "# SO THE ABOVE PROCEDURE PRESERVE THE ABSENCE OF ORDER.\n",
    "# 0-1 doesn't introduce a specific order in the data.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers =[('encoder', OneHotEncoder(), [0])], \n",
    "                       remainder = 'passthrough' ) # don't drop other columns. \n",
    "\n",
    "X = np.array(ct.fit_transform(X)); X # ML model want np.array.\n",
    "\n",
    "#-------- ENCODING BUNARY DATA \n",
    "# TRANSFORM BINARY STRING (YES, NO) INTO {0,1}\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#-------- TRAIN AND TEST SET\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, # independent variables.\n",
    "                                                    y, # dependent variables.\n",
    "                                                    test_size = 0.2, # size of test set.\n",
    "                                                    random_state = 1) # fix the seed for both of data set.\n",
    "\n",
    "\n",
    "#-------- FEATURE SCALING\n",
    "# It must be performed after the splitting. Indeed, this procedure work with\n",
    "# mean and standard deviation. If this technique wuold be applied before, mean and \n",
    "# standard deviation of the all rows impact on the scaling of the training set\n",
    "# which as a consequence, is not independent from the test set. We wuold grab some information\n",
    "# from the test to train. We want to prevent leaking information. \n",
    "\n",
    "# feature selection avoid the domination of some features over other ones\n",
    "# because values are very different.\n",
    "\n",
    "# We can't scale the whole dataset because the LEAKAGE: the test set is supposed to be introduced\n",
    "# after the introduction of the training set. \n",
    "\n",
    "# Why I can's scale irrespectively: Imagine using the training mean and variance to scale the training set and test mean \n",
    "# and variance to scale the test set. Then, for example, a single test example with a value of 1.0 in a particular feature \n",
    "# would have a different original value than a training example with a value of 1.0 (because they were scaled differently), \n",
    "# but would be treated identically by the model. This is where the bias would come from.\n",
    "\n",
    "# There are two techniques: standardization and normalization. The latter work if most of \n",
    "# features are normal distributed, while the former works well in all of situations. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# this procedure isn't applied to binary variables.\n",
    "# Indeed, we'll obtain meaningless values.\n",
    "X_train[:,3:] = sc.fit_transform(X_train[:,3:])\n",
    "# fit compute the mean and devStd.\n",
    "# transform applies the computation.\n",
    "\n",
    "# it's used the same fit of the train set.\n",
    "X_test[:,3:] = sc.transform(X_test[:,3:])\n",
    "# However, this procedure make the Gradient Descent much faster.\n",
    "\n",
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
